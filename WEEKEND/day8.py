# -*- coding: utf-8 -*-
"""Day8

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pP5M2B2EysMtB6VaKKJU33CsdW5XafFm
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
# Generate sample data
X, y = make_blobs(n_samples=300, centers=4, random_state=42)
# Apply K-Means
kmeans = KMeans(n_clusters=4, random_state=42)
y_kmeans = kmeans.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', alpha=0.7)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=300, c='red', label='Centroids', marker='X')
plt.legend()
plt.title("K-Means Clustering")
plt.show()

from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as# Compute cumulative explained variance
pca_full = PCA().fit(X_scaled)
cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)
# Plot cumulative variance
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA: Choosing the Optimal Number of Components")
plt.axhline(y=0.95, color='r', linestyle='--')  # 95% threshold
plt.show()
To select K components that retain 95% vari sch
# Create dendrogram
plt.figure(figsize=(8, 5))
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.title("Dendrogram")
plt.show()
# Apply Agglomerative Clustering
hc = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)
# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_hc, cmap='rainbow', alpha=0.7)
plt.title("Hierarchical Clustering")
plt.show()

# Compute cumulative explained variance
pca_full = PCA().fit(X_scaled)
cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)
# Plot cumulative variance
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA: Choosing the Optimal Number of Components")
plt.axhline(y=0.95, color='r', linestyle='--')  # 95% threshold
plt.show()

from sklearn.cluster import DBSCAN
# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
y_dbscan = dbscan.fit_predict(X)
# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma', alpha=0.7)
plt.title("DBSCAN Clustering")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
# Load dataset (Handwritten digits)
digits = load_digits()
X = digits.data
y = digits.target
# Apply PCA (Reduce to 2 dimensions)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
# Scatter plot of PCA results
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("PCA on Handwritten Digits")
plt.show()

from sklearn.manifold import TSNE
# Apply t-SNE (Reduce to 2D)
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)
# Scatter plot of t-SNE results
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='Spectral', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("t-SNE on Handwritten Digits")
plt.show()

import umap
# Apply UMAP (Reduce to 2D)
umap_reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = umap_reducer.fit_transform(X)
# Scatter plot of UMAP results
plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='coolwarm', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("UMAP on Handwritten Digits")
plt.show()

import tensorflow as tf
from tensorflow import keras
# Define Autoencoder Model
input_dim = X.shape[1]
encoding_dim = 32  # Reduced dimension
# Encoder
input_layer = keras.layers.Input(shape=(input_dim,))
encoded = keras.layers.Dense(encoding_dim, activation='relu')(input_layer)
# Decoder
decoded = keras.layers.Dense(input_dim, activation='sigmoid')(encoded)

# Compile Autoencoder
autoencoder = keras.models.Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')
# Train Autoencoder
autoencoder.fit(X, X, epochs=20, batch_size=256, shuffle=True, verbose=1)
# Get encoded representation
encoder = keras.models.Model(input_layer, encoded)
X_autoencoded = encoder.predict(X)
# Scatter plot of Autoencoder results (first 2 dimensions)
plt.scatter(X_autoencoded[:, 0], X_autoencoded[:, 1], c=y, cmap='plasma', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("Autoencoder Representation of Digits")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
 # Generate synthetic data
X, y = make_blobs(n_samples=300, centers=4, random_state=42)
 # Plot the dendrogram
plt.figure(figsize=(8, 5))
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.title("Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Euclidean Distance")
plt.show()

hc = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_hc, cmap='rainbow', alpha=0.7)
plt.title("Hierarchical Clustering")
plt.show()

# Apply Agglomerative Clustering
hc = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict( # Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=y_hc, cmap='rainbow', alpha=0.7)
plt.title("Hierarchical Clustering")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler# Load the digits dataset
digits = load_digits()
X = digits.data  # Features (64-dimensional)
y = digits.target  # Labels (digits 0-9)
# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
# Scatter plot of PCA results
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(label="Digit Label")
plt.title("PCA: Handwritten Digits (2D Projection)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()